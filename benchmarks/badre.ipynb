{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark experiment on Deep Neural Network based PRSs\n",
    "This notebook collects all the code needed to run the benchmark experiment on the algorithm named DNN-Badre in *Massi, Franco et al., Learning High-Order Interactions for Polygenic Risk Prediction (2022)*.\n",
    "\n",
    "The original paper by Badre et al. can be accessed from here: https://www.nature.com/articles/s10038-020-00832-7. The neural network model is implemented according to the specifics thereby detailed.\n",
    "\n",
    "*Remark* aside from the hiprs package, **this notebook requires the Pytorch library** to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import torch # Pytorch library for building DNN models\n",
    "from hiprs import snps # Auxiliary library for data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNP1</th>\n",
       "      <th>SNP2</th>\n",
       "      <th>SNP3</th>\n",
       "      <th>SNP4</th>\n",
       "      <th>SNP5</th>\n",
       "      <th>SNP6</th>\n",
       "      <th>SNP7</th>\n",
       "      <th>SNP8</th>\n",
       "      <th>SNP9</th>\n",
       "      <th>SNP10</th>\n",
       "      <th>SNP11</th>\n",
       "      <th>SNP12</th>\n",
       "      <th>SNP13</th>\n",
       "      <th>SNP14</th>\n",
       "      <th>SNP15</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SNP1  SNP2  SNP3  SNP4  SNP5  SNP6  SNP7  SNP8  SNP9  SNP10  SNP11  \\\n",
       "0        1     1     0     0     1     2     2     2     1      1      0   \n",
       "1        1     1     0     2     2     2     1     1     1      1      0   \n",
       "2        0     0     0     2     0     2     0     2     2      0      1   \n",
       "3        1     1     0     2     2     2     1     0     0      0      1   \n",
       "4        2     2     0     0     2     0     1     0     2      2      0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...    ...   \n",
       "1495     1     1     0     0     2     2     2     0     0      2      1   \n",
       "1496     2     2     0     0     0     2     0     2     1      0      0   \n",
       "1497     1     1     0     1     1     0     0     2     2      0      2   \n",
       "1498     2     2     0     1     1     2     1     2     0      1      2   \n",
       "1499     2     2     0     1     0     2     2     2     0      1      0   \n",
       "\n",
       "      SNP12  SNP13  SNP14  SNP15  Outcome  \n",
       "0         2      1      0      0        0  \n",
       "1         0      0      0      1        1  \n",
       "2         2      0      0      0        0  \n",
       "3         0      2      2      2        1  \n",
       "4         1      1      1      2        0  \n",
       "...     ...    ...    ...    ...      ...  \n",
       "1495      2      1      2      2        0  \n",
       "1496      1      2      1      1        0  \n",
       "1497      1      1      2      0        1  \n",
       "1498      2      0      0      2        0  \n",
       "1499      2      2      1      0        0  \n",
       "\n",
       "[1500 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data generation\n",
    "seed = 5 # Refers to the simulated dataset analyzed in the paper for the purpose of model interpretability (cf. Fig. 5)\n",
    "#In general, seeds 0 to 29 correspond to the 30 datasets analyzed in the paper. \n\n",
    "ntrain, ntest = 1000, 500 # Number of observations\n",
    "p = 15 # Number of SNPs\n",
    "\n",
    "dataset = snps.generate(n = ntrain + ntest, p = p, noise = 0.01, seed = seed) \n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auxiliary functions and classes for handling DNNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiprs.scores import Classifier, Clock\n",
    "\n",
    "class DNN(Classifier):\n",
    "    def __init__(self, model):\n",
    "        self.dnn = model\n",
    "        self.time = 0\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.dnn(x).view(-1).cpu().numpy()\n",
    "    def fittingtime(self):\n",
    "        return self.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import numpy\n",
    "\n",
    "# Cross-Entropy loss\n",
    "def lossfunction(ypredicted, ytrue):\n",
    "    return -(ytrue*(ypredicted+1e-10).log() + (1.0-ytrue)*(1.0-ypredicted+1e-10).log()).sum()\n",
    "\n",
    "# For model evaluation\n",
    "def error(ypredicted, ytrue):    \n",
    "    return (ypredicted - ytrue).abs().mean() # L1 error\n",
    "\n",
    " # Trains a model on the given dataset. NB: assumes the covariates are in the first columns while the target is in the last one\n",
    "def train(model, train_data, test_data, lossf, optim, lr, epochs, minibatches = None):    \n",
    "    dnn = model.dnn\n",
    "    ntrain, ntest = len(train_data), len(test_data)               \n",
    "\n",
    "    if(minibatches == None):\n",
    "        minibatches = ntrain\n",
    "\n",
    "    optimizer = optim(dnn.parameters(), lr = lr)\n",
    "\n",
    "    def feedback(epoch, mret, mrev):\n",
    "        clear_output(wait=True)\n",
    "        print(\"%s\\nEpoch\\tTrain Error\\tTest Error\" % extra)\n",
    "        print(\"%d\\t%.2e\\t%.2e\" % (epoch, mret, mrev))\n",
    "\n",
    "    timer = Clock()\n",
    "    indexes = list(numpy.arange(ntrain))\n",
    "    timer.start()\n",
    "\n",
    "    terrors = []\n",
    "    verrors = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        shuffle(indexes)\n",
    "        batches = [indexes[(i*minibatches):((i+1)*minibatches)] for i in range(ntrain//minibatches)]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mret = error(dnn(train_data[:,:-1]), train_data[:,[-1]]).item() # Training error\n",
    "            mrev = error(dnn(test_data[:,:-1]) , test_data[:,[-1]] ).item() # Test error\n",
    "            terrors.append(mret)\n",
    "            verrors.append(mrev)\n",
    "            feedback(e, mret, mrev)\n",
    "\n",
    "        for minibatch in batches:\n",
    "            x = train_data[minibatch, :-1]\n",
    "            y = train_data[minibatch,[-1]]\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = lossf(dnn(x), y)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "\n",
    "    timer.stop()\n",
    "    with torch.no_grad():\n",
    "        mret = error(dnn(train_data[:,:-1]), train_data[:,[-1]]).item() # Training error\n",
    "        mrev = error(dnn(test_data[:,:-1]) , test_data[:,[-1]] ).item() # Test error\n",
    "        terrors.append(mret)\n",
    "        verrors.append(mrev)\n",
    "        feedback(e, mret, mrev)\n",
    "    print(\"Training complete. Elapsed time: %s.\" % (timer.elapsedTime()))\n",
    "    model.time = timer.elapsed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model fitting and results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training and test splitting\n",
    "ntrain, ntest = 1000, 500\n",
    "tdata, vdata = data.iloc[:ntrain,:], data.iloc[ntrain:,:]\n",
    "\n",
    "# SNPs data are dummified to allow a proper DNN elaboration of the input\n",
    "train_data = pd.get_dummies(tdata.astype('category')).values\n",
    "test_data = pd.get_dummies(vdata.astype('category')).values\n",
    "mask = [True]*train_data.shape[1]\n",
    "mask[-2] = False\n",
    "\n",
    "# Trasfering data over GPU for faster computations\n",
    "train_data = torch.tensor(train_data[:,mask], dtype = torch.float, device = torch.device(\"cuda:0\"))\n",
    "test_data  = torch.tensor(test_data[:,mask],  dtype = torch.float, device = torch.device(\"cuda:0\"))\n",
    "\n",
    "# Deep Neural Network construction\n",
    "dnn = torch.nn.Sequential(torch.nn.Linear(train_data.shape[1]-1, 100),\n",
    "                          torch.nn.LeakyReLU(0.2),\n",
    "                          torch.nn.Linear(100, 25),\n",
    "                          torch.nn.LeakyReLU(0.2),\n",
    "                          torch.nn.Linear(25, 5),\n",
    "                          torch.nn.LeakyReLU(0.2),\n",
    "                          torch.nn.Linear(5, 1),\n",
    "                          torch.nn.sigmoid())\n",
    "\n",
    "# He initialization of the network model\n",
    "for layer in dnn:\n",
    "    if(isinstance(layer, torch.nn.Linear)):\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='leaky_relu', a = 0.2)\n",
    "\n",
    "# Trasfering the DNN model over GPU\n",
    "dnn.cuda()\n",
    "\n",
    "badre = DNN(dnn)\n",
    "\n",
    "# Optimization\n",
    "train(badre, train_data, test_data, lossfunction, torch.optim.Adam, lr = 1e-3, minibatches = 10, epochs = 200)\n",
    "\n",
    "auc  = badre.auc(test_data[:,:-1], test_data[:,-1].cpu().numpy())\n",
    "time = badre.fittingtime()\n",
    "\n",
    "print(\"Model trained. Elapsed time: %.2f seconds.\" % time)\n",
    "print(\"AUC (test set): %.2f.\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
