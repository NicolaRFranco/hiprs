{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark experiment on Behravan Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook collects all the code needed to run the benchmark experiment on the algorithm named SVM-Behravan in the paper [1].\n",
    "\n",
    "The paper can be accessed from here: https://doi.org/10.1038/s41598-018-31573-5\n",
    "\n",
    "The original source code can be accessed here: https://github.com/hambeh/breast-cancer-risk-prediction\n",
    "\n",
    "For this implementation we kept the original source code but we updated it in Python 3.\n",
    "Details on the hyperparameters' choices implemented in the following can be found in subection Benchmark Algorithms within Materials and Methods section.\n",
    "\n",
    "\n",
    "### References\n",
    "[1] Behravan, H., Hartikainen, J.M., Tengstr√∂m, M. et al. Machine learning identifies interacting genetic variants contributing to breast cancer risk: A case study in Finnish cases and controls. Sci Rep 8, 13149 (2018). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from hiprs import* # Library for high-order interaction aware Polygenic Risk Scores\n",
    "import snps # Auxiliary library for the simulation of genomic data\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    " We collected in the following all functions needed to run the experiment as available in the Github repository of the authors of the algorithm to facilitate inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 functions\n",
    "def cal_XGboost(X_train, Y_train, model, x_test, y_test):\n",
    "    # fitting an XGBoost model and returning feature importance (gain)\n",
    "    model_XGboost = clone(model)\n",
    "    eval_set = [(x_test, y_test)]\n",
    "    model_XGboost.fit(X_train, Y_train, verbose = False, early_stopping_rounds=(model_XGboost.n_estimators)/10, eval_metric=\"auc\", eval_set=eval_set)\n",
    "    #print('XGboost done')\n",
    "    return model_XGboost.get_booster().get_score(importance_type='gain')\n",
    "\n",
    "def all_results_SVM(XX_train,YY_train,XX_validation,YY_validation,indices):\n",
    "#    classifier = clone(model)\n",
    "#    print(model)\n",
    "    classifier = svm.SVC(probability=True, random_state=3, kernel='linear', C=1.5, class_weight='balanced')\n",
    "    classifier.fit(XX_train[:,indices], YY_train)\n",
    "    ts_score = classifier.predict_proba(XX_validation[:,indices])\n",
    "    return ts_score[:,1]\n",
    "\n",
    "\n",
    "def cal_XGboost_feature_importance(X_train,Y_train, indices, model, X_test, Y_test):\n",
    "    # # initial sort: algorithm 1 step 1\n",
    "    model_1 = clone(model)\n",
    "    eval_set = [(X_test[:,indices], Y_test)]\n",
    "    model_1.fit(X_train[:,indices], Y_train,  verbose = False, early_stopping_rounds=(model_1.n_estimators)/10, eval_metric=\"auc\", eval_set=eval_set)\n",
    "    # now sort based on gain\n",
    "    scores_key_values = model_1.get_booster().get_score(importance_type='gain')\n",
    "    index_non_zero = list()\n",
    "    for i in range(len(scores_key_values.keys())): # getting indices of used features in xgboost in [0, len(indices)]\n",
    "        index_non_zero.append(np.int64(list(scores_key_values.keys())[i][1:]))# indices of keys\n",
    "    sorted_values = np.argsort(list(scores_key_values.values()))[::-1] # argsorting based on gain and getting corresponding top indices.\n",
    "    from_top_temp = indices[np.array(index_non_zero)[sorted_values]]   # in range [0,125041]\n",
    "    zir_from_top = np.array(list(set(indices)^set(indices[np.array(index_non_zero)[sorted_values]])))\n",
    "    from_top = np.concatenate((from_top_temp, zir_from_top), axis=0)\n",
    "    return from_top\n",
    "\n",
    "# implementation of algorithm 1\n",
    "def second_cal_XGboost_feature_importance4(XX_train,YY_train, SNPs_indices_sorted_main, M, K, N, model, X_test, Y_test):\n",
    "    SNPs_indices_sorted= SNPs_indices_sorted_main.copy()\n",
    "    model1 = clone(model)\n",
    "    model2 = clone(model)\n",
    "    temp = []\n",
    "\n",
    "    for i in K:\n",
    "\n",
    "        if(M+i <= len(SNPs_indices_sorted)//2):\n",
    "            eval_set1 = [(X_test[:,SNPs_indices_sorted[:M+i]], Y_test)]\n",
    "            model1.fit(XX_train[:,SNPs_indices_sorted[:M+i]], YY_train, verbose = False, early_stopping_rounds=(model1.n_estimators)/10, eval_metric=\"auc\",\n",
    "            eval_set=eval_set1) # from top\n",
    "\n",
    "            eval_set2 = [(X_test[:,SNPs_indices_sorted[-M-i:]], Y_test)]\n",
    "            model2.fit(XX_train[:,SNPs_indices_sorted[-M-i:]], YY_train, verbose = False, early_stopping_rounds=(model2.n_estimators)/10, eval_metric=\"auc\",\n",
    "            eval_set=eval_set2)# from bottom\n",
    "\n",
    "            scores4_key_values = model1.get_booster().get_score(importance_type='gain')\n",
    "            index_non_zero4 = list()\n",
    "            for uu in range(len(scores4_key_values.keys())): # getting indices of used features in xgboost in [0, len(indices)]\n",
    "                index_non_zero4.append(np.int64(list(scores4_key_values.keys())[uu][1:]))# indices of keys\n",
    "            sorted_values4 = np.argsort(list(scores4_key_values.values()))[::-1] # argsorting based on gain and getting corresponding top indices.\n",
    "            M_top = []\n",
    "            M_top_temp = SNPs_indices_sorted[:M+i][np.array(index_non_zero4)[sorted_values4]]\n",
    "            zir_top = np.array(list(set(SNPs_indices_sorted[:M+i])^set(SNPs_indices_sorted[:M+i][np.array(index_non_zero4)[sorted_values4]])))\n",
    "            M_top = np.concatenate((M_top_temp, zir_top), axis=0)\n",
    "\n",
    "            scores5_key_values = model2.get_booster().get_score(importance_type='gain')\n",
    "            index_non_zero5 = list()\n",
    "            for uu in range(len(scores5_key_values.keys())): # getting indices of used features in xgboost in [0, len(indices)]\n",
    "                index_non_zero5.append(np.int64(list(scores5_key_values.keys())[uu][1:]))# indices of keys\n",
    "            sorted_values5 = np.argsort(list(scores5_key_values.values()))[::-1] # argsorting based on gain and getting corresponding top indices.\n",
    "            M_bottom_temp = SNPs_indices_sorted[-M-i:][np.array(index_non_zero5)[sorted_values5]]\n",
    "            zir_bottom = np.array(list(set(SNPs_indices_sorted[-M-i:])^set(SNPs_indices_sorted[-M-i:][np.array(index_non_zero5)[sorted_values5]])))\n",
    "            M_bottom = np.concatenate((M_bottom_temp, zir_bottom), axis=0)\n",
    "\n",
    "            # replace m top with m bottom  rankked snps\n",
    "            SNPs_indices_sorted[:M+i] = M_top # resorting based on new M-top\n",
    "            SNPs_indices_sorted[len(SNPs_indices_sorted)-M-i:len(SNPs_indices_sorted)] = M_bottom # resoritng based on new M-bottom\n",
    "\n",
    "            SNPs_indices_sorted[M+i-N:M+i] = M_bottom[0:N] # exchange N best M_bottom with N worst M_top \n",
    "            SNPs_indices_sorted[len(SNPs_indices_sorted)-M-i:len(SNPs_indices_sorted)-M-i+N] = M_top[M+i-N:M+i] # exchange N worst M_top with N best M_bottom \n",
    "            temp = SNPs_indices_sorted\n",
    "\n",
    "    return temp\n",
    "\n",
    "def Tune_stage2(xgboost_scores, X_train, Y_train, X_test, Y_test, model): # From test NOT CV\n",
    "\n",
    "    model_Tune_stage2 = clone(model)\n",
    "    average_index_non_zero = list()\n",
    "    for i in range(len(xgboost_scores.keys())): #getting indices of selected features from training set. Indices are in [0,125041]\n",
    "        average_index_non_zero.append(np.int64(list(xgboost_scores.keys())[i][1:]))\n",
    "\n",
    "    MM = [2] # window size (Algorithm 1 step 2)\n",
    "    K_increament = [1] # adaptively increase window size (Algorithm 1 step 5)\n",
    "    NN = [1] # let's fix N. algorithm 1 step 4 \n",
    "    global_returned_sorted = list()\n",
    "    tot_roc = list()\n",
    "    # let's fix m, n and k\n",
    "    SNPs_indices_sorted_main = cal_XGboost_feature_importance(X_train, Y_train, np.array(average_index_non_zero), model_Tune_stage2, X_test, Y_test)# initial sorting\n",
    "    SNPs_indices_sorted_main = np.int64(SNPs_indices_sorted_main)\n",
    "    for M in MM:\n",
    "        for KK in K_increament:\n",
    "            for N in NN:\n",
    "                # initial N=sort\n",
    "                K = []\n",
    "                K = list(map(lambda x: x*KK , list(range(len(SNPs_indices_sorted_main)//2))))\n",
    "                returned_sorted4 = second_cal_XGboost_feature_importance4(X_train, Y_train, SNPs_indices_sorted_main, M, K, N, model_Tune_stage2, X_test, Y_test)\n",
    "                if(len(returned_sorted4)):\n",
    "                    selected = np.int64(((np.array(list(range(100)))+1)/float(100))*len(returned_sorted4)) # 1% 2%\n",
    "                    selected = np.unique(selected)\n",
    "                    for ii in selected:\n",
    "                        if(ii!=0):\n",
    "#                            print(ii)\n",
    "                            ts_score1=all_results_SVM(X_train,Y_train, X_test, Y_test, returned_sorted4[:ii])\n",
    "                            # specific M, K, N, ii and CV\n",
    "                            #print('M: ' + str(M)+ ' K: ' + str(KK) + ' N: ' + str(N) + ' ii: ' +  str(ii))\n",
    "                            global_returned_sorted.append(returned_sorted4[:ii])\n",
    "                            precision2, recall2, _ = precision_recall_curve(Y_test, ts_score1)\n",
    "                            tot_roc.append(auc(recall2, precision2))\n",
    "#                            print(auc(recall2, precision2))\n",
    "                            \n",
    "\n",
    "    best_indices_auc_recall = global_returned_sorted[np.argsort(tot_roc)[::-1][0]]\n",
    "    return best_indices_auc_recall\n",
    "\n",
    "def build_XGboost():\n",
    "    model_x = XGBClassifier(nthread=1,seed=0,n_estimators=100,max_depth=2,learning_rate=0.01,subsample=0.4)\n",
    "    return model_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm \n",
    "\n",
    "Running this cell performs all the 30 experiments on simulated data to produce the results reported in the paper. At each loop a new simulated dataset is generated via the custom `generate()` function, available in the `hiprs` packege, using one of the 30 seeds set for the experiments on hiPRS algorithm as well. Generated datasets are saved in the directory of the present notebook where they can be accessed by the R code running the other benchmark experiment (`glinternet.R`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing seed = 0\n",
      "Executing seed = 1\n",
      "Executing seed = 2\n",
      "Executing seed = 3\n",
      "Executing seed = 4\n",
      "Executing seed = 5\n",
      "Executing seed = 6\n",
      "Executing seed = 7\n",
      "Executing seed = 8\n",
      "Executing seed = 9\n",
      "Executing seed = 10\n",
      "Executing seed = 11\n",
      "Executing seed = 12\n",
      "Executing seed = 13\n",
      "Executing seed = 14\n",
      "Executing seed = 15\n",
      "Executing seed = 16\n",
      "Executing seed = 17\n",
      "Executing seed = 18\n",
      "Executing seed = 19\n",
      "Executing seed = 20\n",
      "Executing seed = 21\n",
      "Executing seed = 22\n",
      "Executing seed = 23\n",
      "Executing seed = 24\n",
      "Executing seed = 25\n",
      "Executing seed = 26\n",
      "Executing seed = 27\n",
      "Executing seed = 28\n",
      "Executing seed = 29\n",
      "Train Average precision: 22.263664235408662std: 0.047861906406610616\n",
      "Val Average precision: 23.434269786963867std: 0.053491525596715155\n",
      "Test Average precision: 20.99510344729047std: 0.04236904586802405\n",
      "Average Execution runtime for each dataset: 10.086 seconds\n"
     ]
    }
   ],
   "source": [
    "seeds = list(range(0,30))\n",
    "\n",
    "avg_prec_training_final = list()\n",
    "avg_prec_dev_final = list()\n",
    "avg_prec_test_final = list()\n",
    "avg_auc_test_final = list()\n",
    "tot_runtime = list()\n",
    "selected_snps = list()\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "\n",
    "    tot_average_precisionTR = list()\n",
    "    tot_average_precisionDev = list()\n",
    "    tot_average_precisionTS = list()\n",
    "    best_indices_cvt_auc_recall = list()\n",
    "    tot_AUC_TS = list()\n",
    "\n",
    "    Data = generate(1500, 15, 0.01, seed=seed)\n",
    "    \n",
    "    Data.to_csv(\"hiprs_simdata_%d.csv\" % seed)\n",
    "\n",
    "    x = Data.iloc[:1000,:-1].values\n",
    "    y = Data.iloc[:1000,-1].values\n",
    "    x_cv = Data.iloc[1000:,:-1].values\n",
    "    y_cv = Data.iloc[1000:,-1].values\n",
    "\n",
    "    print('Executing seed = %d' %(seed))\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Fix model for all 5 cvs\n",
    "    # defining an XGBoost model from obtained optimal hyperparameters\n",
    "    model = XGBClassifier(nthread=1,seed=0,n_estimators=500,max_depth=6,learning_rate=0.01,subsample=0.3)\n",
    "\n",
    "    best_indices_cv_auc_recall = list()\n",
    "    # Important: same train and test split as xgboost optimization codes  by fixing random seed\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for train, test in cv.split(x, y):  \n",
    "        X_train = x[train]\n",
    "        Y_train = y[train]\n",
    "        X_test = x[test]\n",
    "        Y_test = y[test]\n",
    "        xgboost_scores1 = cal_XGboost(X_train, Y_train, model, X_test, Y_test)\n",
    "        best_indices_au_recall = Tune_stage2(xgboost_scores1,X_train,Y_train,X_test,Y_test,model)\n",
    "        best_indices_cv_auc_recall.append(best_indices_au_recall)\n",
    "\n",
    "    best_indices_cvt_auc_recall.append(best_indices_cv_auc_recall)\n",
    "\n",
    "    best_indices = best_indices_cvt_auc_recall\n",
    "\n",
    "    indices_new = []\n",
    "    temp = list()\n",
    "    for j in range(len(best_indices)):\n",
    "        for k in range(len(best_indices[j])):\n",
    "    #        temp.append(len(best_indices[j][k]))\n",
    "            indices_new.append(list(best_indices[j][k]))\n",
    "\n",
    "    indices_new1 = np.unique(np.concatenate(indices_new))\n",
    "    \n",
    "    selected_snps.append(indices_new)\n",
    "\n",
    "    counter = -1\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    for train, test in cv.split(x, y):  # accessing train and test for stage 2 parameter tuning \n",
    "        X_train = x[train]\n",
    "        Y_train = y[train]\n",
    "        X_test = x[test]\n",
    "        Y_test = y[test]\n",
    "        counter = counter+1\n",
    "        if(len(indices_new[counter])):\n",
    "\n",
    "             # training\n",
    "            ts_scoreL1=all_results_SVM(X_train, Y_train, X_train, Y_train, indices_new[counter])           \n",
    "            tot_average_precisionTR.append(average_precision_score(Y_train, ts_scoreL1))             \n",
    "             # val\n",
    "            ts_scoreL1=all_results_SVM(X_train, Y_train, X_test, Y_test, indices_new[counter])           \n",
    "            tot_average_precisionDev.append(average_precision_score(Y_test, ts_scoreL1))               \n",
    "             # test             \n",
    "            ts_scoreL1=all_results_SVM(X_train, Y_train, x_cv, y_cv, indices_new[counter])           \n",
    "            tot_average_precisionTS.append(average_precision_score(y_cv, ts_scoreL1))\n",
    "            tot_AUC_TS.append(roc_auc_score(y_cv, ts_scoreL1))\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    runtime = round(end_time - start_time, 3)\n",
    "    \n",
    "    tot_runtime.append(runtime)\n",
    "    \n",
    "    avg_prec_training_in = np.mean(tot_average_precisionTR)\n",
    "    avg_prec_dev_in = np.mean(tot_average_precisionDev)\n",
    "    avg_prec_test_in = np.mean(tot_average_precisionTS)\n",
    "    avg_auc_test_in = np.mean(tot_AUC_TS)\n",
    "    \n",
    "    avg_prec_training_final.append(avg_prec_training_in)\n",
    "    avg_prec_dev_final.append(avg_prec_dev_in)\n",
    "    avg_prec_test_final.append(avg_prec_test_in)\n",
    "    avg_auc_test_final.append(avg_auc_test_in)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "ds = pd.DataFrame([avg_prec_test_final, avg_auc_test_final, tot_runtime]).T\n",
    "ds.columns = ['avg_test_precision', 'avg_test_AUC', 'runtime_sec']\n",
    "ds.to_csv('Behravan_results_NEW.csv')\n",
    "\n",
    "pd.DataFrame(selected_snps).to_csv('selected_snps_behravan_NEW.csv')\n",
    "\n",
    "    \n",
    "print(str('Train Average precision: ')  + str(np.mean(avg_prec_training_final)*100)+ str('std: ') + str(np.std(avg_prec_training_final)))\n",
    "print(str('Val Average precision: ')  + str(np.mean(avg_prec_dev_final)*100)+ str('std: ') + str(np.std(avg_prec_dev_final)))\n",
    "print(str('Test Average precision: ')  + str(np.mean(avg_prec_test_final)*100)+ str('std: ') + str(np.std(avg_prec_test_final)))\n",
    "print('Average Execution runtime for each dataset: %.3f seconds' % np.mean(tot_runtime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
